{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "NB03_NeuralNets.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chenjie-UTS/UTS_ML2019-ID12769194/blob/master/NB03_NeuralNets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w52ilutUCa5",
        "colab_type": "text"
      },
      "source": [
        "# 1 Basic Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh48fEPwUCa6",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Prepare Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVUTU9rjUCa6",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.1 Library Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRXDSL7fUCa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "Zs6JSG2dUCa9",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.2 Visualisation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "lAxot3-VUCa-",
        "colab_type": "text"
      },
      "source": [
        "#### A Simple visualiser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "fhnum9e5UCa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_visualise(mod):\n",
        "    xx, yy = np.meshgrid(np.arange(-5, 5.01, 0.05), \n",
        "                         np.arange(-5, 5.01, 0.05))\n",
        "    xx = xx.flatten()\n",
        "    yy = yy.flatten()\n",
        "    d = pd.DataFrame(data=dict(x=xx, y=yy, \n",
        "                               z=[mod.forward((x, y)) \n",
        "                                  for x, y in zip(xx, yy)]))\n",
        "    fig = px.scatter(d, x='x', y='y', color='z')\n",
        "    fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wg130tRUCbA",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Neural Networks Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN8l4UDyUCbB",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.1 Definition and Naive Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbuCeexkUCbB",
        "colab_type": "text"
      },
      "source": [
        "Let us literally translate the definition of a neural network into computer implementation:\n",
        "Neural network: Multiple _layers_ of _perceptron(s)_.\n",
        "```python\n",
        "def compute_neural_network(x):\n",
        "    # 0. prepare the input for the first layer\n",
        "    layer_input = x\n",
        "    for layer_idx in [0, 1, 2]:\n",
        "        # 1. fill output of this layer by executing each\n",
        "        #    perceptron in this layer\n",
        "        layer_output = []\n",
        "        for perceptron in net_layers[layer_idx]:\n",
        "            perceptron.compute_output(layer_input) \n",
        "            # Note all perceptrons in this layer share the same\n",
        "            # `layer_input`\n",
        "        #!!----------------------------------    \n",
        "        # 2. pass the output of THIS layer\n",
        "        #    to the NEXT layer as the input\n",
        "        #------------------------------------\n",
        "        layer_input = layer_output\n",
        "        # END OF LOOP OVER `layer_idx`\n",
        "```\n",
        "Recall that a perception is to get the weighted sum of all attributes in an input, followed by some _activation_. See below:\n",
        "```python\n",
        "def compute_perceptron(x):\n",
        "    weighted_sum = sum([xi * wi for xi, wi in zip(x, weights)])\n",
        "    return activation_function(weighted_sum)\n",
        "```\n",
        "Of course, we will need to get the weights and the activation function setup. So we will use an object class to represent both the perceptrons and the networks.\n",
        "\n",
        "NB: If you don't understand the construction `[t for t in list_of_t_values]`, please checkout tutorials about Python list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "Z1QMYZACUCbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_func(a):\n",
        "    return 1 / (1 + math.exp(-a))\n",
        "\n",
        "\n",
        "\n",
        "class Perceptron2D:\n",
        "    \"\"\"Perceptron model: linearly combine data attributes followed by a non-linear activation\n",
        "    This is a simplified implementation and deals with data with 2 attributes. \n",
        "    You can also refer to the more complete implementation in the note of Week 3.\n",
        "    \"\"\"\n",
        "    def __init__(self, w0=1, w1=0, activation_func=sigmoid_func):\n",
        "        self.w0 = w0\n",
        "        self.w1 = w1\n",
        "        self.act = activation_func\n",
        "    \n",
        "    def forward(self, x):\n",
        "        wsum = x[0] * self.w0 + x[1] * self.w1\n",
        "        sigmoid_wsum = self.act(wsum) # sigmoid for activation\n",
        "        return sigmoid_wsum\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GODTXZ5XUCbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's have a look at how the perceptron worked on 2D data\n",
        "p = Perceptron2D(0.1, -0.5)\n",
        "simple_visualise(p)\n",
        "# Please note the effect of activation by examining the z-value."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUJOhBWpUCbJ",
        "colab_type": "text"
      },
      "source": [
        "__EXERCISE__\n",
        "\n",
        "In the code cell above, adjust the model parameters and observe the change of the model behaviour."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyIwfxtaUCbK",
        "colab_type": "text"
      },
      "source": [
        "__EXERCISE__\n",
        "\n",
        "Use a different activation function. Such as\n",
        "$$\n",
        "\\begin{align}\n",
        "y(h) = \\left\\{ \\begin{array}{c}\n",
        "0, \\textrm{ if } h \\leq 0 \\\\\n",
        "h, \\textrm{ if } h > 0\n",
        "\\end{array} \\right.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Add implement your activation like:\n",
        "```python\n",
        "def relu_func(h):\n",
        "    # compute y\n",
        "    # HINT: consider using `max`\n",
        "    return y\n",
        "```\n",
        "Then use your function to construct a Perceptron check the behaviour of the perceptron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJWmN3evUCbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_activation_func(h):\n",
        "    return h\n",
        "p = Perceptron2D(0.1, -0.5, activation_func=my_activation_func)\n",
        "simple_visualise(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTP1EbFhUCbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def k2(x):\n",
        "    return [xi**2 for xi in x]\n",
        "\n",
        "class Perceptron2DX:\n",
        "    \"\"\"Perceptron model wrapped. We transform the input before\n",
        "    processing them using the perceptron.\n",
        "    \"\"\"\n",
        "    def __init__(self, percep, xtransform=k2):\n",
        "        self.perceptron = percep\n",
        "        self.xtransform = xtransform\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.perceptron.forward(self.xtransform(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s00Np9AYUCbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pp = Perceptron2DX(p)\n",
        "simple_visualise(pp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPBoMCovUCbR",
        "colab_type": "text"
      },
      "source": [
        "__EXERCISE__\n",
        "\n",
        "__1__:\n",
        "Use a different transform function. Such as\n",
        "$$\n",
        "\\begin{align}\n",
        "x'_1 = \\sin(\\omega_1 \\cdot x_1) \\\\\n",
        "x'_2 = \\cos(\\omega_2 \\cdot x_2)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "__2__:\n",
        "Using your transform function on a _different perceptron core_, e.g.\n",
        "```python\n",
        "vanilla_perceptron_2 = Perceptron2D(\n",
        "        -1, 1, \n",
        "        activation_func=my_activation_func)\n",
        "pp2a = Perceptron2DX(vanilla_perceptron_2, \n",
        "                     xtransform=new_transform)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Yi6j8xVFUCbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def new_transform(x):\n",
        "    tx0 = math.sin(x[0] * 5)\n",
        "    tx1 = math.cos(x[1])\n",
        "    return [tx0, tx1]\n",
        "\n",
        "pp2 = Perceptron2DX(p, xtransform=new_transform)\n",
        "simple_visualise(pp2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "eXNpW_lfUCbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ly0_p0 = Perceptron2D(+1, -1, activation_func=my_activation_func)\n",
        "ly0_p1 = Perceptron2D(-1, -3, activation_func=my_activation_func)\n",
        "ly1_p = Perceptron2D(-1, 0.5, activation_func=math.tanh)\n",
        "def new_transform(x):\n",
        "    tx0 = ly0_p0.forward(x)\n",
        "    tx1 = ly0_p1.forward(x)\n",
        "    return [tx0, tx1]\n",
        "\n",
        "pp3 = Perceptron2DX(ly1_p, xtransform=new_transform)\n",
        "simple_visualise(pp3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT9U35ihUCbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try yourself: can you change the parameters so \n",
        "ly0_p0 = Perceptron2D(+1, -1, activation_func=lambda x:x)\n",
        "ly0_p1 = Perceptron2D(-1, -3, activation_func=lambda x:x)\n",
        "ly1_p = Perceptron2D(-1, 0.5, activation_func=math.tanh)\n",
        "def new_transform(x):\n",
        "    tx0 = ly0_p0.forward(x)\n",
        "    tx1 = ly0_p1.forward(x)\n",
        "    return [tx0, tx1]\n",
        "\n",
        "pp3 = Perceptron2DX(ly1_p, xtransform=new_transform)\n",
        "simple_visualise(pp3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m0DJ9TmUCba",
        "colab_type": "text"
      },
      "source": [
        "## 1.2.2 Multiple Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpM4hZ9mUCbb",
        "colab_type": "text"
      },
      "source": [
        "Alternatively (to the nested perceptrons above), we can define an NeuralNet class to hold all perceptrons in all layers. The advantage is that now we can easily extend the network to have more layers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO5I6XORUCbb",
        "colab_type": "text"
      },
      "source": [
        "#### Naive Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "QgrxLsCjUCbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a class, so our network can manage its \"perceptrons\" easily\n",
        "class NeuralNet:\n",
        "    \"\"\"NeuralNet represents a simple neural network object class.\n",
        "    As an example, it consists of 2 layers of perceptrons. \n",
        "    The first layer has 2 perceptrons and the second one has 1.\n",
        "    \n",
        "    The perceptrons deal with data of 2 attributes.\n",
        "    \"\"\"\n",
        "    def __init__(self, perc0_w=(-1, 1), perc1_w=(2, -1), perc2_w=(0.5, 0.5)):\n",
        "        self.layers = [[Perceptron2D(*perc0_w), Perceptron2D(*perc1_w)], \n",
        "                       [Perceptron2D(*perc2_w)]] # *(w0,w1) expand the values in tuple\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the network output layer by layer. \"forward\" is a conventional\n",
        "        term for execute computing of a net.\n",
        "        \"\"\"\n",
        "        # use layer-0 to process x and get what's to feed to layer-1\n",
        "        layer1_input = [p.forward(x) for p in self.layers[0]]\n",
        "        # get the final output from layer-1\n",
        "        final_output = [p.forward(layer1_input) for p in self.layers[1]]\n",
        "        # note we have only two layers, so I didn't use a loop over the layers\n",
        "        return final_output[0]\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FRvnnjLUCbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net0 = NeuralNet()\n",
        "simple_visualise(net0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LBwem7sUCbg",
        "colab_type": "text"
      },
      "source": [
        "__EXERCISE__ (optional, similar to one above)\n",
        "\n",
        "Adjust parameters to show how the network behaviour changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8U9gwapUCbh",
        "colab_type": "text"
      },
      "source": [
        "#### Computation using Matrix Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hc5Kf4WUCbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def matmul(A, B):\n",
        "    \"\"\"\n",
        "    matmul computs A x B for two matrices\n",
        "    :param A: a collective object contains rows of a matrix\n",
        "        A[i], i-th row, another collective object contains the elements\n",
        "        A[i][j], the element\n",
        "    :param B: similar to A\n",
        "    \"\"\"\n",
        "    # figure out the size of A and B and the result\n",
        "    rows = len(A)\n",
        "    elements_inner = len(A[0])\n",
        "    assert elements_inner == len(B), \"Rows of B must be the same as Cols of A\"\n",
        "    cols = len(B[0])\n",
        "    # Initialise C to the appropriate size\n",
        "    C = [[0.0 for ci in range(cols)] for ri in range(rows)]\n",
        "    \n",
        "    # Fill C: 2 outer loops are for each element\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            # Compute element [i][j]\n",
        "            for k in range(elements_inner):\n",
        "                C[r][c] += A[r][k] * B[k][c]\n",
        "    return C"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq8IOPczUCbj",
        "colab_type": "text"
      },
      "source": [
        "HINT: read the code and try it while watching the accompany video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIYdsnCtUCbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a class, so our network can manage its \"perceptrons\" easily\n",
        "class NeuralNetV2:\n",
        "    \"\"\"NeuralNetV2 represents a simple neural network object class.\n",
        "    This object will manage all the neurons in the network. \n",
        "    \"\"\"\n",
        "    def __init__(self, neuron_numbers_in_layers=[2, 2, 1],\n",
        "                 weights=None):\n",
        "        \"\"\"\n",
        "        :param neuron_numbers_in_layers: first/last -- inputs and outputs\n",
        "        :param weights: a dict, weights[\"in0out1\"] represents the weights\n",
        "          for computing layer1 from layer0, if layer0 has 3 inputs and layer1\n",
        "          has 2 outputs, then the weight will be a matrix of 3 x 2, i.e.\n",
        "          weights[\"in0out1\"][i][j] is the weight for computing element-j in layer1\n",
        "          by using element-i in layer0.\n",
        "        \"\"\"\n",
        "        # Weights between \n",
        "        # Layer1 and 2, Layer 2 and 3, ...\n",
        "        \n",
        "        self.weights = dict()\n",
        "        self.activ_fn = dict()\n",
        "        self.neuron_nums = neuron_numbers_in_layers\n",
        "        self.layer_num = len(neuron_numbers_in_layers) - 1\n",
        "        \n",
        "        # DEFINE (!not DO!, we dont have input X now) computation between layers\n",
        "        for l_in in range(self.layer_num):\n",
        "            l_out = l_in + 1\n",
        "            pkey = f\"in{l_in}out{l_out}\"\n",
        "            try:\n",
        "                # try to use provided weight\n",
        "                W = weights[pkey] # NOTE: should copy\n",
        "            except:\n",
        "                # if not provided ...\n",
        "                n_in = neuron_numbers_in_layers[l_in]\n",
        "                n_out = neuron_numbers_in_layers[l_out]\n",
        "                W = [[random.gauss(0, 0.1) for j in range(n_out)] \n",
        "                     for i in range(n_in)] # see init above\n",
        "            self.weights[pkey] = W\n",
        "            self.activ_fn[pkey] = math.tanh # you may want to try your own\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        DO computations:\n",
        "        \n",
        "        Compute the network output layer by layer. \"forward\" is a conventional\n",
        "        term for execute computing of a net.\n",
        "        :param x: a list of list: x[i], sample-i, having a number of input attributes\n",
        "          if you have only one sample, input it as \n",
        "              [[0, 1, 0]], \n",
        "              NOT [0, 1, 0]\n",
        "        \"\"\"\n",
        "        layer_input = x\n",
        "        for l_in in range(self.layer_num):\n",
        "            # Use layer-in to process x and get what's to feed to layer-out\n",
        "            # Setup \n",
        "            l_out = l_in + 1\n",
        "            pkey = f\"in{l_in}out{l_out}\"\n",
        "            # Compute the weighted sum \n",
        "            layer_pre_activation = matmul(layer_input, self.weights[pkey])\n",
        "            # Perform activation(the construction below is equivalent to nested loops)\n",
        "            layer_out = list(map(lambda x_:list(map(self.activ_fn[pkey], x_)), \n",
        "                            layer_pre_activation))\n",
        "            layer_input = layer_out # feed the output of this layer to the next layer\n",
        "        return layer_out\n",
        "    \n",
        "    \n",
        "class VisWrap:\n",
        "    \"Wrap I/O for the new network object for visualisation\"\n",
        "    def __init__(self, nn):\n",
        "        self.nn = nn\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.nn.forward([x])[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYS7V1skUCbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn2 = NeuralNetV2()\n",
        "nn2_wrap = VisWrap(nn2)\n",
        "simple_visualise(nn2_wrap)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9LkqT8fUCbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's try to adjust the weights. Carefully keep the number of weights \n",
        "# consistent with the number of neurons you had set to the layers.\n",
        "nn2 = NeuralNetV2(\n",
        "    neuron_numbers_in_layers=[2, 3, 1],\n",
        "    weights={\"in0out1\":[[0, 0.5, 1], [-1, -5, 2]],\n",
        "             \"in1out2\":[[-1], [+1], [0.5]]})\n",
        "nn2.activ_fn[\"in1out2\"] = lambda x:max(0, x)\n",
        "nn2_wrap = VisWrap(nn2)\n",
        "simple_visualise(nn2_wrap)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obHRTA-LUCbr",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Training Neural Nets via Backprop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoM3OtvlUCbs",
        "colab_type": "text"
      },
      "source": [
        "It is not trivial to come up with a simple rule to adjust all the parameters in the entire neural network stucture (recall when we consider a single perceptron, we did propose intuitive scheme to improve the fitness of the model to data). \n",
        "\n",
        "The idea is to take a divide-and-conquer scheme. Let's take a careful look at the computation in one perceptron. Throw the machine learning terminology in wind and focus on the computation steps only.\n",
        "$$\n",
        "\\begin{align}\n",
        "a &\\leftarrow w_0 \\cdot x_0 + w_1 \\cdot x_1 \\\\\n",
        "h &\\leftarrow g(a) \\\\\n",
        "Loss &\\leftarrow Compare(h, y)\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad2TNJ2dUCbt",
        "colab_type": "text"
      },
      "source": [
        "Let us think about the statement \"to make the loss smaller\" with a bit care: which specific means we could possibily take to \"make\" the final loss change? During training, we change the model parameters, including $w_{i,j}$. So we need to know the influence on the final loss of each model parameter. In this section we will examine an example of so-called \"backpropagation\" process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqe4e5ZBUCbu",
        "colab_type": "text"
      },
      "source": [
        "### 1.3.1 Adjust Parameters to Modify Model Behaviour"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISZfJ4-HUCbu",
        "colab_type": "text"
      },
      "source": [
        "Given training data $\\{(x_1, y_1), (x_2, y_2), \\dots\\}$, we would like the net to predict for each $x_i$ the target value $y_i$. If this is the case, then our mission has completed. Of course, this is generally NOT the case if we start from a random set of model parameters.\n",
        "\n",
        "For example, if we have one training sample $(x=(2.5, 2), y=1.0)$, let us compare the prediction given by the network above and the target value $y=1.0$: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4snoSyCUCbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First, check the current output of the net\n",
        "def sigm(x):\n",
        "    return 1/(1+math.exp(-x))\n",
        "\n",
        "nn2 = NeuralNetV2(\n",
        "    neuron_numbers_in_layers=[2, 3, 1],\n",
        "    weights={\"in0out1\":[[0, 0.5, 1], [-1, -5, 2]],\n",
        "             \"in1out2\":[[-1], [+1], [0.5]]})\n",
        "nn2.activ_fn[\"in0out1\"] = sigm\n",
        "nn2.activ_fn[\"in1out2\"] = sigm\n",
        "\n",
        "nn2_wrap = VisWrap(nn2)\n",
        "simple_visualise(nn2_wrap)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_DrB23dUCbx",
        "colab_type": "text"
      },
      "source": [
        "Let's check the nets behaviour at one data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6B6nQhHUCbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn2.forward([(2.5, 2)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZUBpvC5UCbz",
        "colab_type": "text"
      },
      "source": [
        "This is smaller than the desired output 1.0. So we want the output to increase at this $x$. Let's learn how to adjust the network using gradients computed through backprop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsgt_1wgUCb0",
        "colab_type": "text"
      },
      "source": [
        "### 1.3.2 Manual Backprop Through Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBSCj4bcUCb0",
        "colab_type": "text"
      },
      "source": [
        "Let us implement the backpropagation for a three layer simple net. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "GBSmMDUNUCb0",
        "colab_type": "text"
      },
      "source": [
        "#### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "G7tCzam4UCb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############################################################\n",
        "# HELPER FUNCTIONS\n",
        "# You do NOT need to learn those to USE modern neural networks\n",
        "# Those functions provide basic array functions in LOW efficiency\n",
        "# but clear manner. You may want to check them if you want to\n",
        "# UNDERSTAND the technical details of NN.\n",
        "# --------\n",
        "# First define sigmoid gradient\n",
        "import math\n",
        "def sigm(x): # redefine here for reference\n",
        "    return 1/(1+math.exp(-x))\n",
        "\n",
        "def gsigmoid(x):\n",
        "    return math.exp(-x)/(1+math.exp(-x))**2\n",
        "\n",
        "def elementwise_apply(f, nested_list):\n",
        "    return list(map(lambda x_:list(map(f, x_)), nested_list))\n",
        "\n",
        "def elementwise_times(nested_list1, nested_list2):\n",
        "    return [[a * b for a, b in zip(r1, r2)] \n",
        "            for r1, r2 in zip(nested_list1, nested_list2)]\n",
        "\n",
        "def mat_tr(nested_list):\n",
        "    return [c for c in zip(*nested_list)]\n",
        "\n",
        "def shape(nested_list):\n",
        "    return len(nested_list), len(nested_list[0])\n",
        "##############################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "HvEZwbwcUCb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# UNIT Test: gsigmoid -- Test the others.\n",
        "test_x = [-3, -1, 0, 1, 3, 5]\n",
        "test_eps = 1e-4\n",
        "for x_ in test_x:\n",
        "    numerical_diff = (sigm(x_ + test_eps) - sigm(x_)) / test_eps\n",
        "    analytic_diff = gsigmoid(x_)\n",
        "    print(f\"NumDiff: {numerical_diff:.3f} ~ AnaDiff: {analytic_diff:.3f}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svbJbpJ1UCb3",
        "colab_type": "text"
      },
      "source": [
        "#### Backprop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FAax_r8UCb4",
        "colab_type": "text"
      },
      "source": [
        "Below I explicily write out the forward method followed by a backward computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mUDgRuaUCb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def special_forward(nn, x):\n",
        "    \"\"\"\n",
        "    This is a special version for manual implementing and testing the backpropagation algorithm. \n",
        "    We only use the network \n",
        "    We don't use the computation and activation of network `nn` \n",
        "    \"\"\"\n",
        "    \n",
        "    # Copy-and-paste and simplify forward computation here:\n",
        "    layer1_input = x\n",
        "    layer1_pre_activation = matmul(layer1_input, nn.weights[\"in0out1\"])\n",
        "    layer1_out = elementwise_apply(sigm, layer1_pre_activation)\n",
        "    \n",
        "    layer2_input = layer1_out # feed the output of this layer to the next layer\n",
        "    \n",
        "    layer2_pre_activation = matmul(layer2_input, nn.weights[\"in1out2\"])\n",
        "    layer2_out = elementwise_apply(sigm, layer2_pre_activation)\n",
        "\n",
        "    layer2_pre_activation_g = elementwise_apply(gsigmoid, layer2_pre_activation)\n",
        "    w12_g = matmul(mat_tr(layer2_input), layer2_pre_activation_g)\n",
        "    layer1_out_g = matmul(layer2_pre_activation_g, mat_tr(nn.weights[\"in1out2\"]))\n",
        "    layer1_pre_activation_g = elementwise_times(\n",
        "        elementwise_apply(gsigmoid, layer1_pre_activation),\n",
        "        layer1_out_g)\n",
        "    w01_g = matmul(mat_tr(layer1_input), layer1_pre_activation_g)\n",
        "\n",
        "    return layer2_out, w01_g, w12_g\n",
        "    \n",
        "# TODO: Mark this somewhere else: this Les Mis is FUNNY! https://www.youtube.com/watch?v=dF495ERjRUo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hohj248UCb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out, w01_g, w12_g = special_forward(nn2, [[2.5, 2]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGh3huCtUCb7",
        "colab_type": "text"
      },
      "source": [
        "#### Numerical verification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE5A0QRzUCb7",
        "colab_type": "text"
      },
      "source": [
        "Next, let us check element by element how does our backprop work. The plan is to adjust each adjustable parameter a bit and check the change of the final output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGcWdVXwUCb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test adjusting weights in0out1\n",
        "wkey = \"in0out1\"\n",
        "w_rows, w_cols = shape(nn2.weights[wkey])\n",
        "numerical_g = [[0 for c in range(w_cols)] for r in range(w_rows)]\n",
        "test_eps = 1e-4\n",
        "test_x = [[2.5, 2]]\n",
        "old_out = nn2.forward(test_x)\n",
        "\n",
        "for r in range(w_rows):\n",
        "    for c in range(w_cols):\n",
        "        old_value = nn2.weights[wkey][r][c] # save the old value to put back after test        \n",
        "        nn2.weights[wkey][r][c] += test_eps\n",
        "        new_out = nn2.forward(test_x)\n",
        "        diff = new_out[0][0] - old_out[0][0] # check the effect of adjusting the corresponding parameter\n",
        "        numerical_g[r][c] = diff / test_eps\n",
        "        # put the old value back\n",
        "        nn2.weights[wkey][r][c] = old_value\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RkoCqkIUCb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"numerical differential\")\n",
        "print(numerical_g)\n",
        "print(\"analytical differential\")\n",
        "print(w01_g)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0DgWKygUCb-",
        "colab_type": "text"
      },
      "source": [
        "__EXECISE__ [Optional]\n",
        "\n",
        "1. Read the code and Explain what you had observed.\n",
        "2. Check the computation for weights transforms data from layer 1 to 2.\n",
        "3. Integrate the backpropagation function into the network class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0cg1cTUUCb_",
        "colab_type": "text"
      },
      "source": [
        "## 1.3.3 Using Computational Framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysVa4MDTUCb_",
        "colab_type": "text"
      },
      "source": [
        "Modern framework allows us to easily perform all the steps above. The example above can be reformulated as"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS8l1zsYUCb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XVWg7U5UCcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyNN(nn.Module):\n",
        "    def __init__(self, neuron_numbers_in_layers=[2, 3, 1]):\n",
        "        super(MyNN, self).__init__()\n",
        "        \n",
        "        self.layers = nn.ModuleList(\n",
        "            [nn.Linear(in_features=nin, out_features=nout)\n",
        "             for nin, nout in zip(neuron_numbers_in_layers[:-1], \n",
        "                                  neuron_numbers_in_layers[1:])])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        for l in self.layers:\n",
        "            h = torch.tanh(l(h))\n",
        "        return h\n",
        "    \n",
        "class TorchVisWrap:\n",
        "    def __init__(self, nn):\n",
        "        self.nn = nn\n",
        "    def forward(self, x):\n",
        "        y = self.nn(torch.Tensor(x).unsqueeze(dim=0))\n",
        "        return y.item()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-7qONAUUCcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(42)\n",
        "nn3 = MyNN([2, 6, 1])\n",
        "nn3_wrap = TorchVisWrap(nn3)\n",
        "simple_visualise(nn3_wrap)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WdZnca_UCcG",
        "colab_type": "text"
      },
      "source": [
        "Let us perform training, call it changing a neural network behaviour or searching in the hypotheses space of neural networks, it is up to your viewpoint. Eg. We want the net to generate\n",
        "    + for (4, -4)\n",
        "    - for (4, 4)\n",
        "    + for (-4, 4)\n",
        "    - for (-4, -4)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0K9PtCKUCcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim import Adam\n",
        "optim = Adam(nn3.parameters(), lr=0.01) # manager: adjust params according to grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRTdCbaVUCcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_steps = 50\n",
        "visualise_every_n_steps = 10\n",
        "trn_X = torch.Tensor([[4, -4], [4, 4], [-4, 4], [-4, -4]])\n",
        "trn_y = torch.Tensor([[1.0], [-1.0], [1.0], [-1.0]])\n",
        "for it in range(train_steps):\n",
        "    loss = ((trn_y - nn3(trn_X))**2).sum()\n",
        "    optim.zero_grad() # reset gradients (to clear computed gradients from previous steps)\n",
        "    loss.backward() # In one stroke, all gradients are computed!\n",
        "    optim.step()  # apply the gradients to the parameters\n",
        "    if it % visualise_every_n_steps == 0:\n",
        "        # Check the effect\n",
        "        simple_visualise(nn3_wrap)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}